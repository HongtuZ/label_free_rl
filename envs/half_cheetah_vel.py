import numpy as np
from gymnasium.envs.mujoco.half_cheetah_v5 import HalfCheetahEnv


class HalfCheetahVelEnv(HalfCheetahEnv):
    """Half-cheetah environment with target velocity, as described in [1]. The
    code is adapted from
    https://github.com/cbfinn/maml_rl/blob/9c8e2ebd741cb0c7b8bf2d040c4caeeb8e06cc95/rllab/envs/mujoco/half_cheetah_env_rand.py

    The half-cheetah follows the dynamics from MuJoCo [2], and receives at each
    time step a reward composed of a control cost and a penalty equal to the
    difference between its current velocity and the target velocity. The tasks
    are generated by sampling the target velocities from the uniform
    distribution on [0, 2].

    [1] Chelsea Finn, Pieter Abbeel, Sergey Levine, "Model-Agnostic
        Meta-Learning for Fast Adaptation of Deep Networks", 2017
        (https://arxiv.org/abs/1703.03400)
    [2] Emanuel Todorov, Tom Erez, Yuval Tassa, "MuJoCo: A physics engine for
        model-based control", 2012
        (https://homes.cs.washington.edu/~todorov/papers/TodorovIROS12.pdf)
    """
    def __init__(self, task, *args, **kwargs):
        super().__init__()
        self.task = task
        # self.tasks = self.sample_tasks(n_tasks)
        self._goal = self.task
        self._step = 0

    def step(self, action):
        x_pos_before = self.data.qpos[0] # 记录上一步的 x 方向位置
        self.do_simulation(action, self.frame_skip) # 模拟若干个 frame
        
        x_pos_after = self.data.qpos[0] # 计算当前 x 方向速度
        x_velocity = (x_pos_after - x_pos_before) / self.dt
        
        # 计算自定义奖励和观测
        forward_reward = -1.0 * abs(x_velocity - self._goal)
        ctrl_cost = 0.5 * 1e-1 * np.sum(np.square(action))
        observation = self._get_obs()
        reward = forward_reward - ctrl_cost
        
        self._step += 1
        done = False
        infos = {
            "forward_reward": forward_reward,
            "reward_ctrl": -ctrl_cost,
            "task" : self.task,
        }
        # if self._step >= self._max_episode_steps: # TODO
            # done = True
        if self.render_mode == "human": # 如果需要窗口渲染
            self.render()
        return observation, reward, done, done, infos # next_obs, reward, terminated, truncated, info

    def reset(self, *, seed=None, options=None):
        task = options.get("task", None) if options else None
        obs, info = super().reset(seed=seed, options=options)
        if task is not None:
            self.task = task
            self._goal = self.task
            self._step = 0
        info = {"task": self.task}
        return obs, info

